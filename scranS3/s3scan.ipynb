{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a120b6f-ba3f-4e09-bbbf-4f0f33fbe30e",
   "metadata": {},
   "source": [
    "### 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8595d96-5c90-407c-8f33-46a0f0de8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8d23c-f2b2-4a41-8cae-beee7165bdf5",
   "metadata": {},
   "source": [
    "### 1. Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b0956d-7281-4904-84c4-8f65858bcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_to_scan = [\"a-mlops-01\", \"a-mlop-shared-01\"]\n",
    "target_bucket = \"a-mlops-01\"  # Where to save output\n",
    "output_prefix = \"s3scan_output\"\n",
    "customer_id_regex = re.compile(r'^2323\\d{4}$')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e805e40-b5ca-44a3-865d-7951d3b4d746",
   "metadata": {},
   "source": [
    "### 2. Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e52f24-28c3-424e-8305-14afa06974f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value = 0\n",
    "unique_values = {}\n",
    "unique_columns = set()\n",
    "unique_file_paths = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717cabd-0404-4873-8883-154f6eb70ecb",
   "metadata": {},
   "source": [
    "### 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c91847-83cf-4e68-bcd5-3442639904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_files(bucket):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    all_keys = []\n",
    "    for page in paginator.paginate(Bucket=bucket):\n",
    "        for obj in page.get('Contents', []):\n",
    "            all_keys.append(obj['Key'])\n",
    "    print(len(all_keys), \" files collected from bucket: \", bucket)\n",
    "    return all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ecc215b-411a-42ac-b608-cebb61ed27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matching_values(df, bucket, key):\n",
    "    global total_value\n",
    "    matched = False\n",
    "    file_uri = f\"{bucket}/{key}\"\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            col_series = df[col].astype(str)\n",
    "            matches = col_series[col_series.str.match(customer_id_regex)]\n",
    "            if not matches.empty:\n",
    "                matched = True\n",
    "                unique_columns.add(col)\n",
    "                unique_file_paths.add(file_uri)\n",
    "                for val in matches:\n",
    "                    total_value += 1\n",
    "                    if val not in unique_values:\n",
    "                        unique_values[val] = set()\n",
    "                    unique_values[val].add(file_uri)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d19009-6ba1-42a9-8df4-0b34746fe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(bucket, key):\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        if key.endswith(\".csv\") or key.endswith(\".txt\"):\n",
    "            df = pd.read_csv(io.BytesIO(body), dtype=str, low_memory=False)\n",
    "            extract_matching_values(df, bucket, key)\n",
    "        elif key.endswith(\".json\"):\n",
    "            df = pd.read_json(io.BytesIO(body), lines=True)\n",
    "            extract_matching_values(df, bucket, key)\n",
    "        elif key.endswith(\".parquet\"):\n",
    "            uri = f\"s3://{bucket}/{key}\"\n",
    "            df = pd.read_parquet(uri, engine='pyarrow')\n",
    "            extract_matching_values(df, bucket, key)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to process {bucket}/{key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e65dc62-5b5a-4b2d-b5ec-1ce4fd3c62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_and_upload_csv(dataframe, bucket, key_name):\n",
    "    buffer = io.StringIO()\n",
    "    dataframe.to_csv(buffer, index=False)\n",
    "    buffer.write(f\"\\ntotalValue : {total_value}\")\n",
    "    buffer.write(f\"\\ntotalUniqueValue : {len(unique_values)}\")\n",
    "    s3.put_object(Bucket=bucket, Key=key_name, Body=buffer.getvalue().encode(\"utf-8\"))\n",
    "    print(f\"‚úÖ Uploaded: s3://{bucket}/{key_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de506e0-5130-4923-89e1-533239a3a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"üîç Starting scan...\")\n",
    "    for bucket in buckets_to_scan:\n",
    "        print(f\"üìÅ Scanning bucket: {bucket}\")\n",
    "        keys = list_all_files(bucket)\n",
    "        for key in keys:\n",
    "            if key.endswith((\".csv\", \".json\", \".parquet\", \".txt\")):\n",
    "                process_file(bucket, key)\n",
    "\n",
    "    print(\"üß† Building output files...\")\n",
    "\n",
    "    # unique_file.csv\n",
    "    data = []\n",
    "    for customer_id, uris in unique_values.items():\n",
    "        data.append({\n",
    "            \"customer_id\": customer_id,\n",
    "            \"S3 URI\": json.dumps(sorted(list(uris)))\n",
    "        })\n",
    "    df_unique_file = pd.DataFrame(data)\n",
    "    write_and_upload_csv(df_unique_file, target_bucket, f\"{output_prefix}/unique_file.csv\")\n",
    "\n",
    "    # unique_columns.csv\n",
    "    df_columns = pd.DataFrame(sorted(unique_columns), columns=[\"column_name\"])\n",
    "    write_and_upload_csv(df_columns, target_bucket, f\"{output_prefix}/unique_columns.csv\")\n",
    "\n",
    "    # unique_file_path.csv\n",
    "    df_paths = pd.DataFrame(sorted(unique_file_paths), columns=[\"S3 URI\"])\n",
    "    write_and_upload_csv(df_paths, target_bucket, f\"{output_prefix}/unique_file_path.csv\")\n",
    "\n",
    "    # Final stats\n",
    "    print(f\"üî¢ totalValue: {total_value}\")\n",
    "    print(f\"üî¢ totalUniqueValue: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30b56833-5c31-428b-b91e-5690559160c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting scan...\n",
      "üìÅ Scanning bucket: a-mlops-01\n",
      "16  files collected from bucket:  a-mlops-01\n",
      "üìÅ Scanning bucket: a-mlop-shared-01\n",
      "5  files collected from bucket:  a-mlop-shared-01\n",
      "üß† Building output files...\n",
      "‚úÖ Uploaded: s3://a-mlops-01/s3scan_output/unique_file.csv\n",
      "‚úÖ Uploaded: s3://a-mlops-01/s3scan_output/unique_columns.csv\n",
      "‚úÖ Uploaded: s3://a-mlops-01/s3scan_output/unique_file_path.csv\n",
      "üî¢ totalValue: 220\n",
      "üî¢ totalUniqueValue: 100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35994f-55ae-47b3-afa4-087227a6a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
