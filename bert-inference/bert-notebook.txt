import boto3
import sagemaker
from sagemaker.huggingface import HuggingFaceModel
from sagemaker import get_execution_role
import pandas as pd
import os

# --- Parameters ---
bucket = "mybucket01"
prefix = "archives"
model_path = "s3://mybucket01/model/model.tar.gz"
test_input = "s3://mybucket01/test/test.csv"
output_path = f"s3://{bucket}/{prefix}/test.csv.out"

role = sagemaker.get_execution_role()
sess = sagemaker.Session()

# Upload test.csv to S3 if needed
if not sess.s3_input(test_input):
    test_df = pd.read_csv("test.csv")
    test_df.to_csv("test_upload.csv", index=False, header=False)
    test_input = sess.upload_data("test_upload.csv", bucket=bucket, key_prefix="test")

# --- Create Model from Existing Artifact ---
huggingface_model = HuggingFaceModel(
    model_data=model_path,
    role=role,
    transformers_version="4.26",
    pytorch_version="1.13",
    py_version="py39",
    entry_point="inference.py",
    source_dir="code",  # must contain inference.py
)

# --- Deploy Model for Batch Transform ---
transformer = huggingface_model.transformer(
    instance_count=1,
    instance_type="ml.m5.xlarge",
    strategy="SingleRecord",
    assemble_with="Line",
    output_path=output_path,
    accept="text/csv",
    env={"SAGEMAKER_PROGRAM": "inference.py"}
)

# Start Batch Transform Job
transformer.transform(
    data=test_input,
    content_type="text/csv",
    split_type="Line",
    input_filter="$[0]",  # optional
    join_source="Input",  # optional: include input + prediction
    output_filter="$[1]"  # just prediction
)

# Wait for completion
transformer.wait()
print(f"âœ… Batch transform completed. Output stored in: {output_path}")
